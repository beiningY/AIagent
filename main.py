import os
from dotenv import load_dotenv
from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader
from langchain.text_splitter import TokenTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")

# åŠ è½½æ–‡ä»¶å¤¹é‡Œçš„æ‰€æœ‰æ–‡æ¡£
def load_documents(folder_path: str):
    loader = DirectoryLoader(
        folder_path,
        loader_cls=UnstructuredFileLoader,
        use_multithreading=True,
        recursive=True
    )
    return loader.load()

# æ–‡æœ¬åˆ‡åˆ†
def split_documents(documents):
    splitter = TokenTextSplitter(
        chunk_size=200,
        chunk_overlap=50
    )
    return splitter.split_documents(documents)

def load_embeddings():
    model_name = "../AgentRag/Camel_agent/models/multilingual-e5-large"

    embeddings = HuggingFaceEmbeddings(
        model_name=model_name,
        encode_kwargs={
            "normalize_embeddings": True  
        }
    )
    return embeddings

# å‘é‡åŒ– + FAISS çŸ¥è¯†åº“æ„å»º
def build_vectorstore(chunks, persist_path: str):
    embeddings = load_embeddings()
    vectorstore = FAISS.from_documents(chunks, embeddings)
    vectorstore.save_local(persist_path)
    return vectorstore

# åŠ è½½å·²æŒä¹…åŒ–çš„ FAISS å‘é‡åº“
def load_vectorstore(persist_path: str):
    #embeddings = OpenAIEmbeddings(model="text-embedding-3-large", dimensions=256)
    embeddings = load_embeddings()
    return FAISS.load_local(
        persist_path,
        embeddings,
        allow_dangerous_deserialization=True
    )

def build_rag(query, vectorstore):
    query = f"query: {query}"
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    retrievers = retriever.invoke(query)
    return retrievers

# QA ç³»ç»Ÿæ„å»º
def build_qa(input,contexts,metadata):
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.4,
        max_tokens=None
    )
    metadata = [i.split("/")[-1] for i in metadata]
    content = ""
    for index, context in enumerate(contexts):
        content += f"{index+1}. {context.page_content}"
        content += "\n"
    content = content.strip()

    messages = [
        (
            "system",
            "ä½ æ˜¯ä¸€ä¸ªé‡‘èä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹ï¼Œç»™å‡ºä¸“ä¸šçš„å›ç­”ã€‚",
        ),
        ("human", f"ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š{input},å¯èƒ½çš„ç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹æ˜¯ï¼š{content}ã€‚å¿…é¡»å‘Šè¯‰ç”¨æˆ·å‚è€ƒçš„æºæ–‡ä»¶æœ‰è¿™å‡ ä¸ªï¼š{metadata}"),
    ]
    return llm, messages
def main(question):

    """print("åŠ è½½å¹¶å¤„ç†æ–‡æ¡£...")
    docs = load_documents(folder_path)
    chunks = split_documents(docs)
    new_chunks = []
    for chunk in chunks:
        chunk.page_content = f"passage: {chunk.page_content}"
        new_chunks.append(chunk)

    print("æ„å»ºå‘é‡åº“å¹¶æŒä¹…åŒ–...")
    build_vectorstore(new_chunks, persist_path)"""
    print("åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ...")
    print(30*"=","å¼€å§‹é—®ç­”")
    query = question.strip()
    print(30*"=","ç”¨æˆ·çš„é—®é¢˜æ˜¯ï¼š",query)
    retrievers = build_rag(query, vectorstore)
    print(30*"=","æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹æ˜¯ï¼š",retrievers)
    metadata = [doc.metadata.get('source') for doc in retrievers]
    llm, messages = build_qa(input=query,contexts=retrievers,metadata=metadata)
    result = llm.invoke(messages)
    output = result.content
    print(30*"=","AIçš„å›ç­”æ˜¯ï¼š",output)
    return output

if __name__ == "__main__":
    import gradio as gr
    folder_path = "bank/bank"
    persist_path = "faiss_index"
    print("åŠ è½½å‘é‡åº“ + æ„å»º ReRank æ£€ç´¢å™¨...")
    vectorstore = load_vectorstore(persist_path)
    print(30*"=","å‘é‡åº“åŠ è½½å®Œæˆ")
    with gr.Blocks(title="Banking Knowledge Question and Answer Assistant") as demo:
        gr.Markdown("## ğŸ’° Banking Knowledge Question and Answer Assistant")
        gr.Markdown("Enter your question about banking knowledge and I will generate professional answers for you based on the knowledge base.")

        with gr.Row():
            with gr.Column():
                user_input = gr.Textbox(label="User Question", placeholder="Please enter your question...", lines=5)
                submit_btn = gr.Button("Submit")
                clear_btn = gr.Button("Clear")

            with gr.Column():
                output = gr.Textbox(label="AI Answer", lines=10, interactive=False)

        submit_btn.click(fn=main, inputs=user_input, outputs=output)
        clear_btn.click(fn=lambda: ("", ""), inputs=[], outputs=[user_input, output])


    demo.launch(
        server_name="0.0.0.0",  # å…è®¸æ‰€æœ‰IPè®¿é—®
        server_port=7860,
        share=True
    )


